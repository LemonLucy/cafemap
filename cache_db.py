import os
import json
import psycopg2
from psycopg2.extras import RealDictCursor

DATABASE_URL = os.getenv('DATABASE_URL')

def get_db_connection():
    """PostgreSQL Ïó∞Í≤∞"""
    if not DATABASE_URL:
        return None
    return psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)

def init_cache_db():
    """Ï∫êÏãú ÌÖåÏù¥Î∏î ÏÉùÏÑ±"""
    conn = get_db_connection()
    if not conn:
        return
    
    try:
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS cafe_cache (
                    id SERIAL PRIMARY KEY,
                    cafe_name VARCHAR(255) NOT NULL,
                    cafe_address VARCHAR(500) NOT NULL,
                    region VARCHAR(100),
                    cache_version VARCHAR(20),
                    result JSONB NOT NULL,
                    hit_count INTEGER DEFAULT 1,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(cafe_name, cafe_address, cache_version)
                )
            """)
            
            # Ïù∏Îç±Ïä§ ÏÉùÏÑ±
            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_cafe_lookup 
                ON cafe_cache(cafe_name, cafe_address, cache_version)
            """)
            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_region 
                ON cafe_cache(region)
            """)
            
            conn.commit()
            print("‚úÖ Cache DB initialized")
    except Exception as e:
        print(f"‚ùå Cache DB init error: {e}")
    finally:
        conn.close()

def should_cache_to_postgres(cafe_address, total_score, hit_count=1):
    """Postgres Ï†ÄÏû• Ïó¨Î∂Ä Í≤∞Ï†ï"""
    # ÏÑúÏö∏ÏùÄ Î¨¥Ï°∞Í±¥ Ï†ÄÏû•
    if 'ÏÑúÏö∏' in cafe_address:
        return True
    
    # Í≤ΩÍ∏∞/Ïù∏Ï≤ú Ï£ºÏöî ÎèÑÏãú
    major_cities = ['ÏàòÏõê', 'ÏÑ±ÎÇ®', 'Í≥†Ïñë', 'Ïö©Ïù∏', 'Î∂ÄÏ≤ú', 'ÏïàÏÇ∞', 'ÏïàÏñë', 'ÎÇ®ÏñëÏ£º', 'ÌôîÏÑ±', 'ÌèâÌÉù', 'Ïù∏Ï≤ú']
    if any(city in cafe_address for city in major_cities):
        return True
    
    # ÏßÄÎ∞© ÎèÑÏãúÎäî Ï°∞Ìöå 2Ìöå Ïù¥ÏÉÅ ÎòêÎäî Í≥†ÎìùÏ†ê
    if hit_count >= 2 or total_score >= 3.0:
        return True
    
    return False

def get_region_from_address(address):
    """Ï£ºÏÜåÏóêÏÑú ÏßÄÏó≠ Ï∂îÏ∂ú"""
    # Ï£ºÏÜå Ï†ïÍ∑úÌôî
    address = address.replace('ÌäπÎ≥ÑÏãú', '').replace('Í¥ëÏó≠Ïãú', '').replace('ÎèÑ', '').strip()
    parts = address.split()
    if len(parts) > 0:
        # "ÏÑúÏö∏ Í∞ïÎÇ®Íµ¨" ‚Üí "ÏÑúÏö∏", "Í≤ΩÍ∏∞ ÏïàÏñëÏãú" ‚Üí "ÏïàÏñë"
        if len(parts) > 1 and 'Ïãú' in parts[1]:
            return parts[1].replace('Ïãú', '').strip()
        return parts[0].strip()
    return 'unknown'

def normalize_address(address):
    """Ï£ºÏÜå Ï†ïÍ∑úÌôî (Ïãú/Íµ¨ Î†àÎ≤®ÍπåÏßÄÎßå, Ï∫êÏãú ÌÇ§ ÎπÑÍµêÏö©)"""
    # "Í≤ΩÍ∏∞ÎèÑ ÏïàÏñëÏãú ÎèôÏïàÍµ¨ Í¥ÄÏñëÎèô 1505" ‚Üí "Í≤ΩÍ∏∞ÏïàÏñë"
    addr = address.replace('ÌäπÎ≥ÑÏãú', '').replace('Í¥ëÏó≠Ïãú', '').replace('ÎèÑ', '').strip()
    parts = addr.split()
    if len(parts) >= 2:
        # Ï≤´ 2Í∞ú Î∂ÄÎ∂ÑÎßå (Ïòà: "Í≤ΩÍ∏∞ ÏïàÏñëÏãú" ‚Üí "Í≤ΩÍ∏∞ÏïàÏñë")
        return ''.join(parts[:2]).replace('Ïãú', '').replace('Íµ¨', '').replace(' ', '')
    elif len(parts) == 1:
        return parts[0].replace('Ïãú', '').replace('Íµ¨', '').replace(' ', '')
    return addr.replace(' ', '')

def get_cached_result(cafe_name, cafe_address, cache_version):
    """Ï∫êÏãúÏóêÏÑú Í≤∞Í≥º Ï°∞Ìöå (Postgres ‚Üí Î©îÎ™®Î¶¨ Ïàú)"""
    # 1Ï∞®: Î©îÎ™®Î¶¨ Ï∫êÏãú
    from app_server import blog_cache
    cache_key = f"{cache_version}_{cafe_name}_{normalize_address(cafe_address)}"
    if cache_key in blog_cache:
        return blog_cache[cache_key]
    
    # 2Ï∞®: Postgres (Ï£ºÏÜå Ï†ïÍ∑úÌôîÌï¥ÏÑú Í≤ÄÏÉâ)
    conn = get_db_connection()
    if not conn:
        return None
    
    try:
        with conn.cursor() as cur:
            # Ï†ïÍ∑úÌôîÎêú Ï£ºÏÜåÎ°ú Í≤ÄÏÉâ
            normalized_addr = normalize_address(cafe_address)
            cur.execute("""
                SELECT result, hit_count, cafe_address FROM cafe_cache 
                WHERE cafe_name = %s AND cache_version = %s
            """, (cafe_name, cache_version))
            
            rows = cur.fetchall()
            for row in rows:
                if normalize_address(row['cafe_address']) == normalized_addr:
                    # Ï°∞ÌöåÏàò Ï¶ùÍ∞Ä
                    cur.execute("""
                        UPDATE cafe_cache 
                        SET hit_count = hit_count + 1, updated_at = CURRENT_TIMESTAMP
                        WHERE cafe_name = %s AND cafe_address = %s AND cache_version = %s
                    """, (cafe_name, row['cafe_address'], cache_version))
                    conn.commit()
                    
                    # Î©îÎ™®Î¶¨ Ï∫êÏãúÏóêÎèÑ Ï†ÄÏû•
                    result = row['result']
                    blog_cache[cache_key] = result
                    return result
    except Exception as e:
        print(f"‚ùå Cache read error: {e}")
    finally:
        conn.close()
    
    return None

def save_cached_result(cafe_name, cafe_address, cache_version, result):
    """Í≤∞Í≥ºÎ•º Ï∫êÏãúÏóê Ï†ÄÏû•"""
    from app_server import blog_cache, MAX_CACHE_SIZE
    
    # Î©îÎ™®Î¶¨ Ï∫êÏãúÏóê Ìï≠ÏÉÅ Ï†ÄÏû• (Ï†ïÍ∑úÌôîÎêú Ï£ºÏÜåÎ°ú)
    cache_key = f"{cache_version}_{cafe_name}_{normalize_address(cafe_address)}"
    if len(blog_cache) >= MAX_CACHE_SIZE:
        oldest_key = next(iter(blog_cache))
        del blog_cache[oldest_key]
    blog_cache[cache_key] = result
    
    # Postgres Ï†ÄÏû• Ïó¨Î∂Ä Í≤∞Ï†ï
    region = get_region_from_address(cafe_address)
    total_score = result.get('totalScore', 0)
    
    if not should_cache_to_postgres(cafe_address, total_score):
        return  # Î©îÎ™®Î¶¨Îßå Ï†ÄÏû•
    
    # PostgresÏóê Ï†ÄÏû•
    conn = get_db_connection()
    if not conn:
        return
    
    try:
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO cafe_cache (cafe_name, cafe_address, region, cache_version, result)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (cafe_name, cafe_address, cache_version) 
                DO UPDATE SET 
                    result = EXCLUDED.result,
                    hit_count = cafe_cache.hit_count + 1,
                    updated_at = CURRENT_TIMESTAMP
            """, (cafe_name, cafe_address, region, cache_version, json.dumps(result)))
            conn.commit()
            print(f"üíæ Cached to Postgres: {cafe_name} ({region})")
    except Exception as e:
        print(f"‚ùå Cache save error: {e}")
    finally:
        conn.close()

def get_cache_stats():
    """Ï∫êÏãú ÌÜµÍ≥Ñ"""
    conn = get_db_connection()
    if not conn:
        return {"error": "DB not connected"}
    
    try:
        with conn.cursor() as cur:
            cur.execute("""
                SELECT 
                    COUNT(*) as total_cafes,
                    SUM(hit_count) as total_hits,
                    COUNT(DISTINCT region) as regions,
                    pg_size_pretty(pg_total_relation_size('cafe_cache')) as db_size
                FROM cafe_cache
            """)
            stats = cur.fetchone()
            
            cur.execute("""
                SELECT region, COUNT(*) as count 
                FROM cafe_cache 
                GROUP BY region 
                ORDER BY count DESC 
                LIMIT 10
            """)
            top_regions = cur.fetchall()
            
            return {
                "stats": dict(stats),
                "top_regions": [dict(r) for r in top_regions]
            }
    except Exception as e:
        return {"error": str(e)}
    finally:
        conn.close()
